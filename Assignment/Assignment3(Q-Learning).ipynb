{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae57e04-b482-4f13-beef-292ef918c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b49092-e5a5-4a5a-8c45-6cdfe9ced95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic States: [0, 1, 2, 3, 4]\n",
      "Actions: ['GREEN', 'RED']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define States and Actions\n",
    "states = [0, 1, 2, 3, 4]  # 0=Empty, 1=Light, 2=Moderate, 3=Heavy, 4=Very Heavy\n",
    "actions = ['GREEN', 'RED']  # Possible actions\n",
    "\n",
    "print(\"Traffic States:\", states)\n",
    "print(\"Actions:\", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2b7d94-62e8-4925-b9d2-effd2588ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Q-Table and Hyperparameters\n",
    "Q = np.zeros((len(states), len(actions)))  \n",
    "\n",
    "alpha = 0.1    \n",
    "gamma = 0.9    \n",
    "epsilon = 0.2  \n",
    "episodes = 300 \n",
    "\n",
    "print(\"Q-table shape:\", Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166a51c3-d6fb-4ebe-8451-7f5bfe4f7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Example (traffic=3, action='GREEN'): 10\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Design Reward Function\n",
    "def get_reward(traffic, action):\n",
    "    if traffic >= 3 and action == 'GREEN':\n",
    "        return 10   # Good: clears congestion\n",
    "    elif traffic >= 3 and action == 'RED':\n",
    "        return -10  # Bad: causes jams\n",
    "    elif traffic == 0 and action == 'RED':\n",
    "        return 5    # Good: saves energy\n",
    "    elif traffic == 0 and action == 'GREEN':\n",
    "        return -5   # Wastes power\n",
    "    elif traffic == 2:\n",
    "        return 1    # Neutral\n",
    "    else:\n",
    "        return 0    # Light traffic or other cases\n",
    "        \n",
    "print(\"Reward Example (traffic=3, action='GREEN'):\", get_reward(3, 'GREEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31287656-3515-4f22-8958-432815fa8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Traffic Example: 2\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define Environment Dynamics\n",
    "def next_traffic(current):\n",
    "    change = random.choice([-1, 0, 1])\n",
    "    return int(np.clip(current + change, 0, 4))\n",
    "    \n",
    "print(\"Next Traffic Example:\", next_traffic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32c9480-c845-4a26-900f-f3974db0786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Agent (Q-learning algorithm)\n",
    "for ep in range(episodes):\n",
    "    traffic = random.choice(states)  \n",
    "    \n",
    "    for _ in range(10):  \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  \n",
    "        else:\n",
    "            action = actions[np.argmax(Q[traffic])] \n",
    "        \n",
    "        next_state = next_traffic(traffic)\n",
    "        reward = get_reward(traffic, action)\n",
    "        \n",
    "        a = actions.index(action)\n",
    "        best_next = np.max(Q[next_state])\n",
    "        Q[traffic, a] += alpha * (reward + gamma * best_next - Q[traffic, a])\n",
    "        traffic = next_state \n",
    "\n",
    "print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de8dcefa-e629-4d98-8d6f-e2b1dc7ca84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter starting traffic level (0–4):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smart Traffic Light Simulation (10 steps) ---\n",
      "Step 1: Traffic Level=3, Light=GREEN\n",
      "Step 2: Traffic Level=2, Light=RED\n",
      "Step 3: Traffic Level=3, Light=GREEN\n",
      "Step 4: Traffic Level=2, Light=RED\n",
      "Step 5: Traffic Level=3, Light=GREEN\n",
      "Step 6: Traffic Level=2, Light=RED\n",
      "Step 7: Traffic Level=2, Light=RED\n",
      "Step 8: Traffic Level=1, Light=GREEN\n",
      "Step 9: Traffic Level=0, Light=RED\n",
      "Step 10: Traffic Level=0, Light=RED\n",
      "\n",
      "Simulation Complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Test the Learned Traffic Light Controller\n",
    "try:\n",
    "    traffic = int(input(\"\\nEnter starting traffic level (0–4): \"))\n",
    "    if traffic < 0 or traffic > 4:\n",
    "        raise ValueError(\"Traffic level out of range!\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    traffic = 2\n",
    "    print(\"Default traffic level set to Moderate (2).\")\n",
    "\n",
    "print(\"\\n--- Smart Traffic Light Simulation (10 steps) ---\")\n",
    "for step in range(10):\n",
    "    action = actions[np.argmax(Q[traffic])]\n",
    "    print(f\"Step {step+1}: Traffic Level={traffic}, Light={action}\")\n",
    "    traffic = next_traffic(traffic)\n",
    "\n",
    "print(\"\\nSimulation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967531d-1245-49da-b697-83fd45818309",
   "metadata": {},
   "source": [
    "###### Step 8: Behavior Analysis (To discuss in your report)\n",
    "Analysis Questions:\n",
    "1. Yes, it learns to do that.\n",
    "2. Yes, it saves energy.\n",
    "3. More exploration, even if it slows, the learning is getting broader\n",
    "4. Gets even faster convergence, but risk of local optimum.\n",
    "5. Values for correct actions (GREEN on heavy, RED on empty) increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053f8b2-081d-4ed1-a8dc-49b878fb65f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
